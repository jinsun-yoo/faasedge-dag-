# Imports
import csv
from tqdm import tqdm
import json
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import os
from collections import defaultdict
import pickle
from datetime import datetime
import numpy as np
from sklearn.neighbors import KDTree
from sklearn.cluster import KMeans
import time

def post_process_links_bw_consumption(links_bandwidth_consumption):
    """Link (a, b) is logically equivalent to link (b, a), keep (a < b) and add to it (b, a)

    Args:
        links_bandwidth_consumption: policy output
    """
    # print(links_bandwidth_consumption)
    filtered = defaultdict(int)
    for key in links_bandwidth_consumption:
        a, b = key
        if a < b:
            filtered[key] = links_bandwidth_consumption[key]
        elif a > b:
            key_ = (b, a)
            filtered[key_] += links_bandwidth_consumption[key]
            # if key_ in filtered:
            #     filtered[key_] += links_bandwidth_consumption[key]
            # else:
            #     filtered[key] = links_bandwidth_consumption[key]
        else:
            pass
            # print(f"Removed erroneous intra-node link: {(a, b)}") # can happen if filter/stitch on same node
    # print(filtered)
    return filtered

def plot_ensemble(links_bw):
    num_subplots = len(links_bw)
    fig, axes = plt.subplots(1, num_subplots, figsize=(num_subplots * 6, 6))
    for i, (links_bandwidth_consumption, title, space) in enumerate(links_bw):
        plot_links_bw_consumption(links_bandwidth_consumption, title, axes[i], space)
    plt.tight_layout()
    plt.show()

def plot_links_bw_consumption(links_bandwidth_consumption, title, ax, space=1):
    print(f"{len(links_bandwidth_consumption.keys())} links used.")
    keys = list(map(lambda x: str(x), links_bandwidth_consumption.keys()))
    values = links_bandwidth_consumption.values()
    
    ax.bar(keys, values, color="crimson")
    # plt.bar(keys, values)
    ax.xaxis.set_major_locator(ticker.MultipleLocator(space)) 

    ax.set_xlabel("Links")
    ax.set_xticklabels(keys, rotation=45)
    ax.set_ylabel("Bandwidth Consumption")
    ax.set_title(title)
    
def plot_cell_tower_ingress(edge_node_bandwidth_consumption):
    plt.bar(range(len(edge_node_bandwidth_consumption)), edge_node_bandwidth_consumption, color="navy")
    plt.xlabel('Edge Node')
    plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))
    plt.ylabel('Bandwidth Consumption')
    plt.title('Bandwidth Consumption of First Hop Edge Nodes')
    plt.show()

def load_sf_cell_towers(file_path, outliers_removed=True):
    """Returns node ids and np.array of nx2 coordinates (x, y) of cell towers

    Args:
        file_path (str): file path to cell towers csv
    """
    
    edge_nodes = None
    with open(file_path, 'r') as f:
        reader = csv.reader(f, delimiter=',')
        rows = [row for row in reader]
        keys = rows[0]      # x: longitude (EW), y: latitude (NS)
        values = [list(map(float, v[:2])) + v[2:] for v in rows[1:]]
        edge_nodes = [ dict(zip(keys, v)) for v in values ]
    
    edge_node_locations = [ (node['id'], [node['x'], node['y']]) for node in edge_nodes]
    
    if outliers_removed:    # manual determination of outliers
        # remove outliers
        del edge_node_locations[2]
        del edge_node_locations[2]
        del edge_node_locations[3]

    node_ids, node_locations_ = list(zip(*edge_node_locations))
    node_locations = np.array(list(node_locations_))
    
    return node_ids, node_locations

def load_sf_cabs_time_index(pickle_file_path, start=0, end=1, write=True):
    """Load a time range of sf cabs [start, end] in number of hours
    
    Usage:
        SF_CABS_TIME_INDEX = "time_index.pkl"
        time_index = load_sf_cabs_time_index(SF_CABS_TIME_INDEX, start=0, end=1)   
    Args:   
        pickle_file_path (str): path to .pkl file generated by microbenchmark.ipynb
    """
    
    loaded_data = None
    # Read the dictionary from the pickle file
    with open('time_index.pkl', 'rb') as pickle_file:
        loaded_data = pickle.load(pickle_file)
        
    keys = list(loaded_data.keys())  # times are in seconds, UNIX epoch
    min_time = min(keys)
    
    time_index = {}
    for key in tqdm(keys):
        if key >= min_time + start * 3600 and key <= min_time + end * 3600:
            time_index[key] = loaded_data[key]
            
    if write:
        with open(f'time_index_{start}_{end}.pkl', 'wb') as pickle_file:
            pickle.dump(time_index, pickle_file)
    
    return time_index

def load_mini_sf_cabs():
    loaded_data = None
    # alias for start=0, end=1
    # with open('mini_time_index.pkl', 'rb') as pickle_file:
    #     loaded_data = pickle.load(pickle_file)
    with open('mini_time_index.pkl', 'rb') as pickle_file:
        loaded_data = pickle.load(pickle_file)
    return loaded_data


####
"""
max concurrent cars 13 in first hour (this analyzes time index)
times = list(time_index.keys())
largest = -1
for a in times:
    t_ = list(time_index[a].keys())
    largest = max(largest, len(t_))
print(largest)
"""
